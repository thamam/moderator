<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>3</epicId>
    <storyId>6</storyId>
    <title>Integrate Ever-Thinker with Learning System</title>
    <status>drafted</status>
    <generatedAt>2025-11-10</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>bmad-docs/stories/3-6-integrate-ever-thinker-with-learning-system.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>Moderator system developer</asA>
    <iWant>Ever-Thinker to integrate with the Learning System to query historical patterns, filter based on acceptance rates, and record improvement outcomes</iWant>
    <soThat>the system learns from every project and continuously improves suggestion quality over time</soThat>
    <tasks>
      - Task 1: Implement recent rejection filtering (AC: 3.6.2)
      - Task 2: Implement IMPROVEMENT_FEEDBACK message handling (AC: 3.6.3)
      - Task 3: Verify acceptance rate query integration (AC: 3.6.1, 3.6.4)
      - Task 4: Implement graceful degradation for Learning System failures (AC: 3.6.5)
      - Task 5: Write comprehensive tests for learning system integration
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="3.6.1">
      <description>Ever-Thinker queries LearningDB.get_acceptance_rates_by_type() before proposing improvements</description>
      <details>
        - Method call integrated into improvement cycle workflow
        - Returns dict mapping improvement types to acceptance rates: {'performance': 0.85, 'code_quality': 0.72, ...}
        - Acceptance rates used in priority scoring algorithm (already implemented in Story 3.5)
      </details>
    </criterion>
    <criterion id="3.6.2">
      <description>Improvements filtered if similar improvement was rejected in last 30 days</description>
      <details>
        - Implement check_recent_rejection() call in improvement cycle
        - Query parameters: improvement_type, target_file, days_back=30
        - If similar improvement rejected recently → skip and log (don't propose)
        - Filter applied after scoring, before PR creation
      </details>
    </criterion>
    <criterion id="3.6.3">
      <description>After receiving IMPROVEMENT_FEEDBACK, Ever-Thinker records outcome via ImprovementTracker.record_acceptance() or record_rejection()</description>
      <details>
        - Implement handle_message() for IMPROVEMENT_FEEDBACK message type
        - Extract improvement_id, accepted (boolean), reason from message payload
        - Call improvement_tracker.record_acceptance() if accepted
        - Call improvement_tracker.record_rejection() if rejected
        - Log outcome with structured logger
      </details>
    </criterion>
    <criterion id="3.6.4">
      <description>Learning System updates reflected in next improvement cycle (acceptance rates adjust based on new data)</description>
      <details>
        - After recording outcome, acceptance rates automatically update in LearningDB (Epic 2 functionality)
        - Next cycle queries fresh acceptance rates (no caching)
        - Priority scores reflect updated historical data
      </details>
    </criterion>
    <criterion id="3.6.5">
      <description>If Learning System unavailable, Ever-Thinker continues without filtering (degraded mode)</description>
      <details>
        - Wrap all learning system calls in try/except
        - On exception: log warning, use default acceptance rate (0.5)
        - On exception for filtering: skip filter (allow proposal)
        - System remains operational without learning features
      </details>
    </criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>bmad-docs/tech-spec-epic-3.md</path>
        <title>Epic 3 Technical Specification</title>
        <section>Story 3.6: Learning System Integration (lines 814-825)</section>
        <snippet>AC 3.6.1-3.6.5 define learning system integration requirements. Includes query patterns, record outcomes, and graceful degradation. Message handling for IMPROVEMENT_FEEDBACK and filtering logic specified.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/tech-spec-epic-3.md</path>
        <title>Epic 3 Technical Specification</title>
        <section>Learning System Integration Patterns (lines 498-541)</section>
        <snippet>Query patterns: get_acceptance_rates_by_type() returns {'performance': 0.85, ...}. Record patterns: record_proposal(), record_acceptance(), record_rejection() with improvement_id and reason parameters.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/tech-spec-epic-3.md</path>
        <title>Epic 3 Technical Specification</title>
        <section>Improvement Cycle Sequence Diagram (lines 547-577)</section>
        <snippet>Sequence: ET→L: Get tasks, ET→A: Run analyzers, ET→L: Filter based on historical data, ET→L: Check recent rejections, ET→MB: Publish IMPROVEMENT_PROPOSAL, M→MB: Publish IMPROVEMENT_FEEDBACK, ET→L: Record outcome, ET→ET: Update acceptance rates.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/epics.md</path>
        <title>Epic 3: Ever-Thinker Continuous Improvement Engine</title>
        <section>Story 3.6 (lines 115-119)</section>
        <snippet>Connect Ever-Thinker to LearningDB to query historical patterns, filter suggestions based on acceptance rates, and update learning data with improvement outcomes. Closes the learning loop.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/stories/3-5-implement-improvement-cycle-orchestration.md</path>
        <title>Story 3.5: Implement Improvement Cycle Orchestration</title>
        <section>Dev Notes - Integration Points (lines 114-119)</section>
        <snippet>Learning System: Queried for historical acceptance rates, updated with outcomes. calculate_priority_score() already queries learning_db.get_acceptance_rate() with fallback to 0.5 on failure.</snippet>
      </doc>
      <doc>
        <path>config/config.yaml</path>
        <title>Gear 3 Configuration</title>
        <section>gear3.ever_thinker (lines relevant to integration)</section>
        <snippet>max_cycles: 3, idle_threshold_seconds: 300, perspectives: [performance, code_quality, testing, documentation, ux, architecture]. Configuration for improvement cycle limits.</snippet>
      </doc>
    </docs>

    <code>
      <artifact>
        <path>src/agents/ever_thinker_agent.py</path>
        <kind>agent</kind>
        <symbol>EverThinkerAgent</symbol>
        <lines>1-545</lines>
        <reason>Main agent class - needs extension for learning system integration. Contains _run_improvement_cycle() (407-545) to add filtering, handle_message() to extend, calculate_priority_score() (307-358) already queries learning system.</reason>
      </artifact>
      <artifact>
        <path>src/agents/ever_thinker_agent.py</path>
        <kind>method</kind>
        <symbol>handle_message</symbol>
        <lines>290-305</lines>
        <reason>Message handler to extend with IMPROVEMENT_FEEDBACK case. Currently placeholder that logs received messages.</reason>
      </artifact>
      <artifact>
        <path>src/agents/ever_thinker_agent.py</path>
        <kind>method</kind>
        <symbol>_run_improvement_cycle</symbol>
        <lines>407-545</lines>
        <reason>Improvement cycle workflow - add filtering step after scoring (line 476), before PR creation (line 479). Insert check_recent_rejection() logic here.</reason>
      </artifact>
      <artifact>
        <path>src/agents/ever_thinker_agent.py</path>
        <kind>method</kind>
        <symbol>calculate_priority_score</symbol>
        <lines>307-358</lines>
        <reason>Already implements AC 3.6.1 - queries learning_db.get_acceptance_rate() with try/except fallback to 0.5. No changes needed, verify only.</reason>
      </artifact>
      <artifact>
        <path>src/learning/learning_db.py</path>
        <kind>database</kind>
        <symbol>LearningDB</symbol>
        <lines>1-end</lines>
        <reason>Provides acceptance rates and pattern data. Methods: get_acceptance_rate(), get_acceptance_rates_by_type(), check_recent_rejection() (if exists, else needs implementation).</reason>
      </artifact>
      <artifact>
        <path>src/learning/improvement_tracker.py</path>
        <kind>tracker</kind>
        <symbol>ImprovementTracker</symbol>
        <lines>69-118, 119-165, 166-210</lines>
        <reason>Records improvement outcomes. Methods: record_proposal() (line 69), record_acceptance() (line 119), record_rejection() (line 166). Use in handle_message() for IMPROVEMENT_FEEDBACK.</reason>
      </artifact>
      <artifact>
        <path>src/communication/messages.py</path>
        <kind>enum</kind>
        <symbol>MessageType.IMPROVEMENT_FEEDBACK</symbol>
        <lines>42</lines>
        <reason>Message type already defined for Gear 3. Use in handle_message() to process moderator feedback on improvement proposals.</reason>
      </artifact>
      <artifact>
        <path>src/communication/messages.py</path>
        <kind>class</kind>
        <symbol>AgentMessage</symbol>
        <lines>1-end</lines>
        <reason>Message data structure with message_type, from_agent, to_agent, payload fields. IMPROVEMENT_FEEDBACK payload: {improvement_id, accepted, reason}.</reason>
      </artifact>
      <artifact>
        <path>src/agents/analyzers/models.py</path>
        <kind>dataclass</kind>
        <symbol>Improvement</symbol>
        <lines>1-end</lines>
        <reason>Improvement data model with improvement_type, target_file, impact, effort fields. Used for filtering and recording. Score field added in Story 3.5.</reason>
      </artifact>
      <artifact>
        <path>src/logger.py</path>
        <kind>logger</kind>
        <symbol>StructuredLogger</symbol>
        <lines>1-end</lines>
        <reason>Logging system for recording degraded mode events, filtering decisions, and feedback handling. Use component='ever-thinker', action='learning_system_degraded'.</reason>
      </artifact>
    </code>

    <dependencies>
      <python>
        <package name="pytest" version=">=7.0.0">Unit testing framework</package>
        <package name="unittest.mock">Mocking for tests (standard library)</package>
        <package name="sqlite3">Database backend for LearningDB (standard library)</package>
        <package name="dataclasses">Data structures for models (standard library)</package>
        <package name="enum">Enumerations for message types (standard library)</package>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>All learning system calls MUST be wrapped in try/except for graceful degradation (AC 3.6.5)</constraint>
    <constraint>Filtering must occur AFTER scoring but BEFORE PR creation in improvement cycle (AC 3.6.2)</constraint>
    <constraint>Default acceptance rate is 0.5 when learning system fails (Story 3.5 pattern)</constraint>
    <constraint>IMPROVEMENT_FEEDBACK message payload must contain: improvement_id, accepted (bool), reason (string)</constraint>
    <constraint>Recent rejection check uses 30-day window (days_back=30) as specified in AC 3.6.2</constraint>
    <constraint>No caching of acceptance rates - fresh query each cycle (AC 3.6.4)</constraint>
    <constraint>Log all degraded mode events with structured logger (component, action, details)</constraint>
    <constraint>Daemon thread must not crash if learning system unavailable - continue without filtering</constraint>
    <constraint>Follow existing error handling patterns from Story 3.5 (try/except with default values)</constraint>
    <constraint>Maintain 100% test pass rate - all 494+ tests must pass after changes</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>ImprovementTracker.record_acceptance</name>
      <kind>method</kind>
      <signature>def record_acceptance(self, improvement_id: int, pr_number: int) -> None</signature>
      <path>src/learning/improvement_tracker.py:119</path>
    </interface>
    <interface>
      <name>ImprovementTracker.record_rejection</name>
      <kind>method</kind>
      <signature>def record_rejection(self, improvement_id: int, reason: str) -> None</signature>
      <path>src/learning/improvement_tracker.py:166</path>
    </interface>
    <interface>
      <name>LearningDB.get_acceptance_rate</name>
      <kind>method</kind>
      <signature>def get_acceptance_rate(self, improvement_type: ImprovementType) -> float</signature>
      <path>src/learning/improvement_tracker.py:266</path>
      <note>Already called by calculate_priority_score() in Story 3.5</note>
    </interface>
    <interface>
      <name>LearningDB.check_recent_rejection</name>
      <kind>method</kind>
      <signature>def check_recent_rejection(self, improvement_type: str, target_file: str, days_back: int = 30) -> bool</signature>
      <path>src/learning/learning_db.py (to be verified or implemented)</path>
      <note>Returns True if similar improvement rejected recently, False otherwise</note>
    </interface>
    <interface>
      <name>MessageType.IMPROVEMENT_FEEDBACK</name>
      <kind>enum_value</kind>
      <signature>IMPROVEMENT_FEEDBACK: Moderator approves/rejects improvement proposal</signature>
      <path>src/communication/messages.py:42</path>
    </interface>
    <interface>
      <name>AgentMessage payload structure</name>
      <kind>data_structure</kind>
      <signature>{"improvement_id": str, "accepted": bool, "reason": str}</signature>
      <path>src/communication/messages.py</path>
      <note>Payload format for IMPROVEMENT_FEEDBACK messages</note>
    </interface>
    <interface>
      <name>StructuredLogger.error/info/warn</name>
      <kind>method</kind>
      <signature>def error/info/warn(self, component: str, action: str, **kwargs)</signature>
      <path>src/logger.py</path>
      <note>Use for logging degraded mode, filtering decisions, and feedback handling</note>
    </interface>
  </interfaces>

  <tests>
    <standards>
      Testing standards from Story 3.5: Class-based test organization (TestRecentRejectionFiltering, TestFeedbackHandling, TestDegradedMode, etc.). Use pytest framework with unittest.mock for mocking. Mock LearningDB, ImprovementTracker, and MessageBus. Verify structured logging calls with mock assertions. All tests must use descriptive names and comprehensive docstrings. Test files located in tests/ directory. Naming convention: test_*.py.
    </standards>

    <locations>
      tests/test_learning_integration.py (new file to create)
      tests/test_ever_thinker_agent.py (may need updates if behavior changes)
      tests/test_improvement_cycle.py (verify no regressions)
    </locations>

    <ideas>
      <idea ac="3.6.1">
        Test that calculate_priority_score() queries learning_db.get_acceptance_rate() (already verified in Story 3.5)
      </idea>
      <idea ac="3.6.1">
        Integration test: verify no caching - acceptance rate changes reflected in next cycle
      </idea>
      <idea ac="3.6.2">
        Test recent rejection filtering: improvement skipped when check_recent_rejection() returns True
      </idea>
      <idea ac="3.6.2">
        Test filtering occurs after scoring but before PR creation (workflow order)
      </idea>
      <idea ac="3.6.2">
        Test multiple improvements filtered vs. some filtered, some allowed
      </idea>
      <idea ac="3.6.3">
        Test IMPROVEMENT_FEEDBACK message handling: acceptance path (calls record_acceptance)
      </idea>
      <idea ac="3.6.3">
        Test IMPROVEMENT_FEEDBACK message handling: rejection path (calls record_rejection)
      </idea>
      <idea ac="3.6.3">
        Test structured logging for feedback handling (component, action, improvement_id, accepted, reason)
      </idea>
      <idea ac="3.6.4">
        Integration test: record outcome → next cycle queries updated rates → priority changes
      </idea>
      <idea ac="3.6.5">
        Test degraded mode: get_acceptance_rate() fails → defaults to 0.5
      </idea>
      <idea ac="3.6.5">
        Test degraded mode: check_recent_rejection() fails → allows proposal (fail open)
      </idea>
      <idea ac="3.6.5">
        Test degraded mode: record_acceptance() fails → logs error, continues
      </idea>
      <idea ac="3.6.5">
        Test degraded mode: record_rejection() fails → logs error, continues
      </idea>
      <idea ac="3.6.5">
        Test degraded mode logging: verify warning messages with clear degradation context
      </idea>
      <idea ac="general">
        Edge case: empty acceptance_rates_by_type() dict
      </idea>
      <idea ac="general">
        Edge case: improvement_type not in acceptance rates dict
      </idea>
    </ideas>
  </tests>
</story-context>
